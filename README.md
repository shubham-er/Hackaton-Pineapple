# Hackaton-Pineapple

1. Load NSE_classification.xlsx in dataset

2. Change path and ibm_api_key_id of object storage bucket in notebook where you see below lines

The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.
client_6a94a2d51103455f82f9a755c24f1ed0 = ibm_boto3.client(service_name='s3',
    ibm_api_key_id='xxxxxxxxxxxxxxxxxxx',
    ibm_auth_endpoint="https://iam.cloud.ibm.com/oidc/token",
    config=Config(signature_version='oauth'),
    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')

body = client_6a94a2d51103455f82f9a755c24f1ed0.get_object(Bucket='lticitihackathonpro-donotdelete-pr-lpkddav7n9w3qp',Key='NSE_classification.xlsx')['Body']

3. Change api_key and location to your Watson Machine Learning instance where you see below lines

api_key = 'xxxxxxxxxxxxxxxx'
location = 'us-south'
wml_credentials = {
    "apikey": api_key,
    "url": 'https://' + location + '.ml.cloud.ibm.com'
}
from ibm_watson_machine_learning import APIClient

4. Create Deployment space and attach WML with it and change space id with your space id where you see below line

space_id = 'xxxxxxxxxxxxxxxxxxx'

5. Save and run notebook. It will train and validate and deploy model.
In the end of notebook python code is written to deploy model as batch job and create and run job to test deployed model with sample input.
Once notebook completed succesfully goto Assets under your deployment space and you can see results.csv in datasets generated by deployed model.
You can check job logs under Jobs tab under your deployment space.
